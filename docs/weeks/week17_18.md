---
layout: default
---
# Weeks 17, 18: Driving video, New Dataset, Number of data for each class, Data statistics, Effect of image colourspace on CNN


## Driving videos

### Lstm_tinypilotnet

I've used the predictions of the lstm_tinypilotnet network (regression network) to driving a formula 1: 

[![Follow line with lstm_tinypilotnet network for w and v (Dataset 3, test1)](https://roboticslaburjc.github.io/2017-tfm-vanessa-fernandez/images/image_simple_circuit.png)](https://www.youtube.com/watch?v=_QW5M-hRgBA)



### Imbalanced classfication network

I've used the predictions of the classification network according to w (7 classes) and constant v to driving a formula 1: 

[![Follow line with classification network for w and constant v (Dataset 3, test1, imbalanced)](https://roboticslaburjc.github.io/2017-tfm-vanessa-fernandez/images/image_simple_circuit.png)](https://www.youtube.com/watch?v=z2l6Eal6ycI)


I've used the predictions of the classification network according to w (7 classes) and v (4 classes) to driving a formula 1: 

[![Follow line with classification network for w and v (Dataset 3, test1, imbalanced)](https://roboticslaburjc.github.io/2017-tfm-vanessa-fernandez/images/image_simple_circuit.png)](https://www.youtube.com/watch?v=ZPTdgUH_8Y4)



### Balanced classfication network

I've used the predictions of the classification network according to w (7 classes) and constant v to driving a formula 1: 

[![Follow line with classification network for w and constant v (Dataset 3, test1, balanced)](https://roboticslaburjc.github.io/2017-tfm-vanessa-fernandez/images/image_simple_circuit.png)](https://www.youtube.com/watch?v=Y_S2oh32XUI)


I've used the predictions of the classification network according to w (7 classes) and v (4 classes) to driving a formula 1: 

[![Follow line with classification network for w and v (Dataset 3, test1, balanced)](https://roboticslaburjc.github.io/2017-tfm-vanessa-fernandez/images/image_simple_circuit.png)](https://www.youtube.com/watch?v=_GQoXatS77s)



### Biased classfication network

I've used the predictions of the classification network according to w (7 classes) and constant v to driving a formula 1: 

[![Follow line with classification network for w and constant v (Dataset 3, test1, biased)](https://roboticslaburjc.github.io/2017-tfm-vanessa-fernandez/images/image_simple_circuit.png)](https://www.youtube.com/watch?v=U5EBQcqQ_jg)


I've used the predictions of the classification network according to w (7 classes) and v (4 classes) to driving a formula 1: 

[![Follow line with classification network for w and v (Dataset 3, test1, biased)](https://roboticslaburjc.github.io/2017-tfm-vanessa-fernandez/images/image_simple_circuit.png)](https://www.youtube.com/watch?v=AoU0iLH-huQ)


## New Dataset

I've based on the code created for the [follow-line practice of JdeRobot Academy](http://vanessavisionrobotica.blogspot.com/2018/05/practica-1-follow-line-prueba-2.html) in order to create a new dataset. This new dataset has been generated using 3 circuits so that the data is more varied. The circuits of the worlds monacoLine.world, f1.launch (simpleCircuit.world) and f1-chrono.launch have been used. Once the dataset is complete, a [file](https://github.com/RoboticsURJC-students/2017-tfm-vanessa-fernandez/blob/master/Follow%20Line/dl-driver/Net/split_train_test.py) has used to divide the data into train and test. It has been decided to divide the dataset by 70% for train and 30% for test. Since the dataset was 17341 pairs of values, we now have 12138 pairs of train values ​​and 5203 pairs of test values. 



## Number of data for each class

I've used a script (evaluate_class.py) that shows a graph of the number of examples that exist for each class (7 classes of w). In the following images we can see the graph for the training data: 

![bar_train_dataset3](https://roboticslaburjc.github.io/2017-tfm-vanessa-fernandez/images/bar_train_dataset3.png)



## Data statistics

To analyze the data, I've analyzed two lines of each image and calculated the centroids of the corresponding lines (row 250 and row 360). On the x-axis of the graph, the centroid of row 350 is represented and the y-axis represents the centroid of row 260 of the image. In the following image we can see the representation of this statistic of the training set (new dataset) (red circles) against the driving data (blue crosses). 

![L1_L2_dataset3](https://roboticslaburjc.github.io/2017-tfm-vanessa-fernandez/images/L1_L2_dataset3.png)



## Effect of image colourspace on CNN

I've read the paper [Effect Of Image Colourspace On Performance Of Convolution Neural Networks](https://ieeexplore.ieee.org/document/8256949). Generally in CNN the processing of images is done in RGB colourspace even though we have many other colourspaces available. In this paper they try to understand the effect of image colourspace on the performance of CNN models in recognizing the objects present in the image. They evaluate this on CIFAR10 dataset, by converting all the original RGB images into four other colourspaces like HLS, HSV, LUV, YUV etc. To compare results they have trained AlexNet (5 convolution layers and 3 fully connected layers) with fixed set of parameters on all five colourspaces, including RGB. They have observed that LUV colourspace is the best alternative to RGB colourspace to use with CNN models with almost equal performance on the test set of CIFAR10 dataset. While YUV colourspace is the worst to use with CNN models.

They've used CIFAR10 dataset, which has 60000 RGB images (32x32x3). The dataset has 60000 images with 10 classes where 50000 images are for training and 10000 images are for testing. They have converted the entire dataset of 60000 RGB colourspace images into four different colourspaces HSL, HSV, LUV, and YUV. In each dataset the training set of 50000 images is further split into two parts for training and validation, where 80% of images are used for training and 20% images are used for validation.

The architecture of AlexNet (5 convolution layers and 3 fully connected layers) that was used for training is: Convolution + MaxPooling + Batch Normalization, Convolution + MaxPooling + Batch Normalization, Convolution, Convolution, Convolution + MaxPooling, Fully Connected, Fully Connected, Fully Connected. They train AlexNet using the datasets separately. The training procedure for each dataset involves preprocessing, hyper-parameter selection and training.

They observed that LUV colourspace is a very good alternative to the RGB colourspace while training CNN model to recognize the objects in the image. Also we have observed that the YUV colourspace is the worst among others. RGB colourspace gives a test accuracy of 67.84% where LUV colourspace gives a test accuracy of 61%. The test accuracy of YUV colourspace is 25% less than the RGB colourspace and 19% less than the LUV colourspace. RGB and LUV have correct prediction along with high confidence scores. But in the case of colourspaces like YUV, HLS and HSV either the prediction is wrong or the confidence of prediction is very less. 
